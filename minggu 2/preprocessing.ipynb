{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae35ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Document Preprocessing - Sistem Temu Kembali Informasi\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Mata Kuliah : Sistem Temu Kembali Informasi\n",
    "# Topik       : Document Preprocessing\n",
    "# Pertemuan   : Minggu 2\n",
    "# Dosen       : Defri Kurniawan, M.Kom\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîç Document Preprocessing - Sistem Temu Kembali Informasi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6985cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "180f843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assalamualaikum Wr. Wb. yang menjadi salah satu syarat untuk bisa ujian kompre ada sertifikat TOEIC,\n",
      "sehingga jika belum lulus toeic maka tidak bisa melakukan ujian kompre. saya rasa ini sangat menghambat\n",
      "teman-teman yang memang lemah dibidang bahasa inggris (atau yang kurang beruntung dalam ujian toeic-nya).\n",
      "sehingga mereka tidak bisa fokus untuk ujian kompre-nya. terima kasih..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Assalamualaikum Wr. Wb. yang menjadi salah satu syarat untuk bisa ujian kompre ada sertifikat TOEIC,\n",
    "sehingga jika belum lulus toeic maka tidak bisa melakukan ujian kompre. saya rasa ini sangat menghambat\n",
    "teman-teman yang memang lemah dibidang bahasa inggris (atau yang kurang beruntung dalam ujian toeic-nya).\n",
    "sehingga mereka tidak bisa fokus untuk ujian kompre-nya. terima kasih..\n",
    "\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "548a45f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HASIL PARSING ===\n",
      "\n",
      "assalamualaikum wr wb yang menjadi salah satu syarat untuk bisa ujian kompre ada sertifikat toeic\n",
      "sehingga jika belum lulus toeic maka tidak bisa melakukan ujian kompre saya rasa ini sangat menghambat\n",
      "temanteman yang memang lemah dibidang bahasa inggris atau yang kurang beruntung dalam ujian toeicnya\n",
      "sehingga mereka tidak bisa fokus untuk ujian komprenya terima kasih\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Menghapus karakter selain huruf dan mengubah ke huruf kecil\n",
    "parsed_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "parsed_text = parsed_text.lower()\n",
    "\n",
    "print(\"=== HASIL PARSING ===\")\n",
    "print(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e125ddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HASIL TOKENISASI ===\n",
      "['assalamualaikum', 'wr', 'wb', 'yang', 'menjadi', 'salah', 'satu', 'syarat', 'untuk', 'bisa', 'ujian', 'kompre', 'ada', 'sertifikat', 'toeic', 'sehingga', 'jika', 'belum', 'lulus', 'toeic', 'maka', 'tidak', 'bisa', 'melakukan', 'ujian', 'kompre', 'saya', 'rasa', 'ini', 'sangat', 'menghambat', 'temanteman', 'yang', 'memang', 'lemah', 'dibidang', 'bahasa', 'inggris', 'atau', 'yang', 'kurang', 'beruntung', 'dalam', 'ujian', 'toeicnya', 'sehingga', 'mereka', 'tidak', 'bisa', 'fokus', 'untuk', 'ujian', 'komprenya', 'terima', 'kasih']\n",
      "Jumlah token: 55\n"
     ]
    }
   ],
   "source": [
    "tokens = parsed_text.split()\n",
    "\n",
    "print(\"=== HASIL TOKENISASI ===\")\n",
    "print(tokens)\n",
    "print(\"Jumlah token:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723c0259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HASIL STOPWORD REMOVAL ===\n",
      "['assalamualaikum', 'wr', 'wb', 'menjadi', 'salah', 'satu', 'syarat', 'ujian', 'kompre', 'sertifikat', 'toeic', 'sehingga', 'belum', 'lulus', 'toeic', 'tidak', 'melakukan', 'ujian', 'kompre', 'rasa', 'sangat', 'menghambat', 'temanteman', 'memang', 'lemah', 'dibidang', 'bahasa', 'inggris', 'yang', 'kurang', 'beruntung', 'ujian', 'toeicnya', 'sehingga', 'tidak', 'bisa', 'fokus', 'ujian', 'komprenya', 'terima', 'kasih']\n",
      "Jumlah token setelah stopword removal: 41\n"
     ]
    }
   ],
   "source": [
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "\n",
    "clean_text = stopword.remove(parsed_text)\n",
    "filtered_tokens = clean_text.split()\n",
    "\n",
    "print(\"=== HASIL STOPWORD REMOVAL ===\")\n",
    "print(filtered_tokens)\n",
    "print(\"Jumlah token setelah stopword removal:\", len(filtered_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7109555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HASIL STEMMING ===\n",
      "['assalamualaikum', 'wr', 'wb', 'jadi', 'salah', 'satu', 'syarat', 'uji', 'kompre', 'sertifikat', 'toeic', 'sehingga', 'belum', 'lulus', 'toeic', 'tidak', 'laku', 'uji', 'kompre', 'rasa', 'sangat', 'hambat', 'temanteman', 'memang', 'lemah', 'bidang', 'bahasa', 'inggris', 'yang', 'kurang', 'untung', 'uji', 'toeicnya', 'sehingga', 'tidak', 'bisa', 'fokus', 'uji', 'komprenya', 'terima', 'kasih']\n",
      "Jumlah token setelah stemming: 41\n"
     ]
    }
   ],
   "source": [
    "stem_factory = StemmerFactory()\n",
    "stemmer = stem_factory.create_stemmer()\n",
    "\n",
    "stemmed_text = stemmer.stem(clean_text)\n",
    "stemmed_tokens = stemmed_text.split()\n",
    "\n",
    "print(\"=== HASIL STEMMING ===\")\n",
    "print(stemmed_tokens)\n",
    "print(\"Jumlah token setelah stemming:\", len(stemmed_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573dbab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Semua hasil preprocessing telah disimpan di 'hasil_preprocessing.txt'\n"
     ]
    }
   ],
   "source": [
    "with open(\"hasil_preprocessing.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== HASIL PARSING ===\\n\")\n",
    "    f.write(parsed_text + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"=== HASIL TOKENISASI ===\\n\")\n",
    "    f.write(str(tokens) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"=== HASIL STOPWORD REMOVAL ===\\n\")\n",
    "    f.write(str(filtered_tokens) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"=== HASIL STEMMING ===\\n\")\n",
    "    f.write(str(stemmed_tokens) + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Semua hasil preprocessing telah disimpan di 'hasil_preprocessing.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb3ff955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Kesimpulan:\n",
      "Tahapan preprocessing teks mencakup:\n",
      "1. Parsing ‚Äî Membersihkan teks dari karakter tidak relevan.\n",
      "2. Tokenisasi ‚Äî Memecah teks menjadi potongan kata.\n",
      "3. Stopword Removal ‚Äî Menghapus kata umum yang tidak bermakna penting.\n",
      "4. Stemming ‚Äî Mengubah kata berimbuhan menjadi bentuk dasarnya.\n",
      "\n",
      "Hasil akhir berupa daftar 'term' yang siap digunakan untuk tahap indexing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "üìñ Kesimpulan:\n",
    "Tahapan preprocessing teks mencakup:\n",
    "1. Parsing ‚Äî Membersihkan teks dari karakter tidak relevan.\n",
    "2. Tokenisasi ‚Äî Memecah teks menjadi potongan kata.\n",
    "3. Stopword Removal ‚Äî Menghapus kata umum yang tidak bermakna penting.\n",
    "4. Stemming ‚Äî Mengubah kata berimbuhan menjadi bentuk dasarnya.\n",
    "\n",
    "Hasil akhir berupa daftar 'term' yang siap digunakan untuk tahap indexing.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
